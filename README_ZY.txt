###20240903 
【0】'--do_predict' 参数默认为false，即默认不跑验证集。

【1】在 Python 中，.format() 方法是一种字符串格式化操作，它允许你构建字符串，并将变量或值插入到字符串中的特定位置。
.format() 方法使用花括号 {} 作为占位符，这些占位符将被 .format() 方法中的参数替换。参数的位置和顺序决定了占位符被替换的顺序。

###20240905
【0】什么是Checkpoint？
在机器学习和深度学习中，checkpoint（检查点）是指在模型训练过程中保存的模型状态。这些检查点通常包括模型的参数（权重和偏置）、优化器状态和其他相关的训练信息。通过保存检查点，您可以在训练过程中定期保存模型的当前状态，以便在需要时恢复训练或用于模型评估和推理。

Checkpoint的组成
一个典型的检查点通常包含以下内容：

模型权重：模型的所有参数，包括权重和偏置。
优化器状态：优化器的状态，包括动量、学习率等。
训练状态：当前的训练轮数（epoch）、批次（batch）编号等。
其他元数据：如学习率调度器的状态、自定义指标等。

创建Checkpoint
在PyTorch中，您可以使用 torch.save 函数保存检查点：

加载Checkpoint
要恢复训练或进行推理，您可以使用 torch.load 和 load_state_dict 函数：
原文链接：https://blog.csdn.net/Flemington7/article/details/139026499

【1】EarlyStopping 是一个用于在训练过程中提前结束训练的机制，以避免过拟合。当模型在验证集上的性能在连续多个周期（epochs）内没有显著改进（验证损失）时，该机制会触发停止训练。

【2】在深度学习中，优化器（Optimizer）扮演着至关重要的角色。它负责在模型训练过程中更新网络参数，以最小化损失函数并提高模型的性能。以下是优化器的几个关键作用和特点：

### 1. 参数更新
优化器根据损失函数的梯度来更新模型的参数。在训练过程中，通过前向传播计算损失，然后通过反向传播计算损失函数关于模型参数的梯度。优化器使用这些梯度来调整参数，目的是减少损失。

### 2. 学习率控制
优化器通常包含一个学习率参数，它决定了参数更新的步长。学习率是深度学习中最重要的超参数之一。如果学习率设置得太高，可能会导致训练过程中的损失振荡或发散；如果太低，则训练过程可能会变得非常缓慢。

### 3. 动量和自适应学习率
许多优化器如SGD（随机梯度下降）的变种（如Adam、RMSprop等）引入了动量（Momentum）或自适应学习率的概念。动量可以帮助优化器在相关方向上加速收敛，并抑制无关方向上的波动。自适应学习率则根据参数更新的历史情况动态调整每个参数的学习率，这有助于优化训练过程。

### 4. 优化算法
不同的优化器基于不同的优化算法，这些算法有不同的特性和适用场景。例如：
- **SGD**：最基本的优化器，使用简单的梯度下降方法。
- **Adam**：结合了动量和自适应学习率，通常在多种任务中表现良好。
- **RMSprop**：通过调整学习率来解决梯度消失或爆炸的问题。
- **Adagrad**：适用于稀疏数据，通过为每个参数定制学习率来优化。

### 5. 影响训练动态
优化器的选择和配置可以显著影响模型的训练动态，包括收敛速度、最终性能和训练稳定性。选择合适的优化器和调整其参数是实现高效训练的关键步骤。

### 示例代码
在 PyTorch 中，优化器的使用通常如下所示：

```python
import torch.optim as optim

model = MyModel()
optimizer = optim.Adam(model.parameters(), lr=0.001)

for data, target in dataloader:
    optimizer.zero_grad()   # 清零梯度
    output = model(data)      # 前向传播
    loss = loss_function(output, target)  # 计算损失
    loss.backward()          # 反向传播
    optimizer.step()         # 更新参数
```

在这个例子中，`Adam` 优化器被用来更新 `MyModel` 的参数。每次迭代后，通过调用 `optimizer.step()` 来应用梯度更新。

### 总结
优化器是深度学习训练过程中不可或缺的部分，它负责根据损失函数的梯度来调整模型参数，以期达到模型性能的最优化。正确选择和配置优化器对于训练效果至关重要。

【3】
自动混合精度（AMP）
目的：自动混合精度是一种用于加速深度学习模型训练的技术，同时减少内存使用和提高计算效率。通过结合单精度（float32）和半精度（float16）的计算，可以在支持的硬件（如NVIDIA的较新GPU）上实现这些优势。

torch.cuda.amp.GradScaler：这是 PyTorch 提供的一个工具，用于在自动混合精度训练中自动缩放梯度。由于使用半精度计算可能会导致梯度数值过小（下溢），GradScaler 通过动态缩放梯度来防止这个问题，确保训练过程的稳定性。

###20240910
#1 可视化输出训练过程的参数，测试结果可视化
#2 用BHM数据集测试模型

###20240912
【0】
torch.cat 是 PyTorch 中的一个函数，用于将多个张量（tensors）沿着已存在的维度进行拼接。这是处理张量时非常常用的操作，尤其是在需要合并数据或特征时。

函数的基本语法如下：

python
torch.cat(tensors, dim=0, *, out=None) → Tensor
参数解释：

tensors：一个张量序列，即一个列表或元组，包含了要进行拼接的张量。所有张量必须有相同的形状，除了在dim指定的维度以外。
dim：指定拼接的维度，默认为0。它的值不能超过张量的最大维数。
out：输出张量，可选参数。如果提供，拼接的结果将被写入这个张量中。
返回值：

返回一个新的张量，它是输入张量沿着指定维度拼接的结果。
